{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7a3da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "from transformers import TFBertModel\n",
    "import os\n",
    "import tensorflow_addons as tfa\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e510258",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"C:\\Users\\siddh\\Downloads\\train_watson.csv\"\n",
    "train_df = pd.read_csv(\"train.csv\")a\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a0d0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"C:\\Users\\siddh\\Downloads\\test.csv~\"\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99cfcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d0cacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"entailment\",\"neutral\",\"contradiction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c03306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = random.randint(0,len(train_df)-5)    # create random index not more than \n",
    "for row in train_df[[\"premise\",\"hypothesis\",\"label\"]][random_index:random_index+5].itertuples():\n",
    "    _, premise, hypothesis, label = row    # _ is to get rid of index\n",
    "    \n",
    "    if label == 0: print(f\"label: {label}\", \"{entailment}\")\n",
    "    elif label == 1: print(f\"label: {label}\", \"{neutral}\")\n",
    "    elif label == 2: print(f\"label: {label}\", \"{contradictory}\")\n",
    "    print(f\"-> Premise: {premise}\\n-> Hypothesis: {hypothesis}\\n\")\n",
    "    print(\"-\"*30,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da019df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(train_df[[\"premise\",\"hypothesis\"]].to_numpy(),\n",
    "                                                                    train_df[\"label\"].to_numpy(),\n",
    "                                                                    test_size = 0.1,\n",
    "                                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f2826",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0], train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656eb8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "def encode_sentence(s):\n",
    "    tokens = list(tokenizer.tokenize(s))\n",
    "    tokens.append('[SEP]')\n",
    "    return tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f382dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the tokenize function\n",
    "\n",
    "print('Original sentence: ' + train_df.premise[0] + '\\n')\n",
    "print('Tokenized sentence: ' + str(encode_sentence(train_df.premise[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fd093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode data for the bert model with a max length of 100\n",
    "\n",
    "def bert_encode(hypotheses, premises, tokenizer, max_length=100):\n",
    "\n",
    "    x = [h + ' [SEP] ' + p for h, p in zip(np.array(hypotheses), np.array(premises))]\n",
    "    x = tokenizer(x, padding=True, truncation=True, max_length=max_length)\n",
    "\n",
    "    inputs = {\n",
    "          'input_word_ids':tf.ragged.constant(x['input_ids']).to_tensor(),\n",
    "          'input_mask': tf.ragged.constant(x['attention_mask']).to_tensor(),\n",
    "          'input_type_ids': tf.ragged.constant(x['token_type_ids']).to_tensor()}\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e61b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = bert_encode(train_df.premise.values, train_df.hypothesis.values, tokenizer)\n",
    "\n",
    "train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa64588",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = bert_encode(test_df.premise.values, test_df.hypothesis.values, tokenizer)\n",
    "\n",
    "test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678b3286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable TPU for faster training\n",
    "\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except ValueError:\n",
    "    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n",
    "    print('Number of replicas:', strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2273a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from official.nlp import optimization\n",
    "os.environ[\"WANDB_API_KEY\"] = \"0\" # to silence warning...sometimes :D\n",
    "\n",
    "max_len = 100\n",
    "bert_encoder = TFBertModel.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86e0e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp,base_model=bert_encoder):\n",
    "    '''\n",
    "    Keras tunes model using bert_encoder(base_multilingual_cased) and finding out the optimal hyperparameters\n",
    "    '''\n",
    "    base_model.trainable = False\n",
    "    \n",
    "# INPUTS\n",
    "    input_word_ids = tf.keras.Input(shape=(max_len,),dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = tf.keras.Input(shape=(max_len,),dtype=tf.int32, name=\"input_mask\")\n",
    "    input_type_ids = tf.keras.Input(shape=(max_len,),dtype=tf.int32, name=\"input_type_ids\")\n",
    "    \n",
    "    x = base_model([input_word_ids, input_mask, input_type_ids])[0]\n",
    "    x = tf.keras.layers.LSTM(units=hp.Int('lstm_', min_value=50, max_value=100), return_sequences=True)(x)\n",
    "    for i in range(hp.Int('num_dense_layers', 1, 5)):\n",
    "        x = tf.keras.layers.Dropout(hp.Choice('dropout_', values=[0.0, 0.1, 0.2]))(x)\n",
    "        x = tf.keras.layers.Dense(units=hp.Int('dense_', min_value=50, max_value=100), activation='relu')(x)\n",
    "\n",
    "# OUTPUT\n",
    "    output = tf.keras.layers.Dense(3, activation='softmax')(x[:,0,:])\n",
    "\n",
    "# BUILD THE FRAMEWORK\n",
    "    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n",
    "\n",
    "# OPTIMIZER\n",
    "    step = tf.Variable(0, trainable=True)\n",
    "    schedule = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    [5000, 7500,10000,12500], [1e-0, 5e-1, 1e-1, 5e-2,1e-2])\n",
    "    lr = 1e-3 * schedule(step)\n",
    "    wd = lambda: 1e-4 * schedule(step)\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd)\n",
    "\n",
    "# COMPILE\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss= tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "                  metrics=[\"accuracy\",f1])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8609c40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train BERT model\n",
    "\n",
    "tuner = kt.tuners.BayesianOptimization(build_model,\n",
    "                                        seed=42,\n",
    "                                        objective='val_loss',\n",
    "                                        max_trials=5,\n",
    "                                        directory='.',\n",
    "                                        project_name = \"My_dear_Watson1\")\n",
    "\n",
    "tuner.search(train_input,\n",
    "            train_df.label.values,\n",
    "            epochs = 12,\n",
    "            verbose = 1,\n",
    "            batch_size = 64,\n",
    "            validation_split = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec5423",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(1)[0]\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6535f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_number, layer in enumerate(best_model.layers):\n",
    "    print(f\"{layer_number}  {layer.name} : {layer.trainable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc008527",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n",
    "best_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d60b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(train_input,\n",
    "            train_df.label.values,\n",
    "            epochs = 15,\n",
    "            verbose = 1,\n",
    "            batch_size = 64,\n",
    "            validation_split = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d48106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_class(sample_premise,sample_hypothesis,tokenizer):\n",
    "    '''\n",
    "    Predicts classes for the sample input\n",
    "    '''\n",
    "    \n",
    "    global classes\n",
    "    sample_input = bert_encode(sample_premise,sample_hypothesis,tokenizer)\n",
    "    pred = model.predict(sample_input)\n",
    "    return classes[tf.argmax(pred,axis=1)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
